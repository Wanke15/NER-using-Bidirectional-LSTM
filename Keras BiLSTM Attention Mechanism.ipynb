{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (6,6)\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\n\nfrom keras.layers import Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed\nfrom keras.layers import Dropout, Embedding, GlobalMaxPooling1D, MaxPooling1D, Add, Flatten, SpatialDropout1D\nfrom keras.layers import GlobalAveragePooling1D, BatchNormalization, concatenate\nfrom keras.layers import Reshape, merge, Concatenate, Lambda, Average\nfrom keras.models import Sequential, Model, load_model\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import Constant\nfrom keras.layers.merge import add\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.utils import np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['news-category-dataset', 'glove6b']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# load data\n\ndf = pd.read_json('../input/news-category-dataset/News_Category_Dataset.json', lines=True)\ndf.head()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "           authors                        ...                                                          short_description\n0  Melissa Jeltsen                        ...                          She left her husband. He killed their children...\n1    Andy McDonald                        ...                                                   Of course it has a song.\n2       Ron Dicker                        ...                          The actor and his longtime girlfriend Anna Ebe...\n3       Ron Dicker                        ...                          The actor gives Dems an ass-kicking for not fi...\n4       Ron Dicker                        ...                          The \"Dietland\" actress said using the bags is ...\n\n[5 rows x 6 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>authors</th>\n      <th>category</th>\n      <th>date</th>\n      <th>headline</th>\n      <th>link</th>\n      <th>short_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Melissa Jeltsen</td>\n      <td>CRIME</td>\n      <td>2018-05-26</td>\n      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n      <td>She left her husband. He killed their children...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Andy McDonald</td>\n      <td>ENTERTAINMENT</td>\n      <td>2018-05-26</td>\n      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n      <td>Of course it has a song.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ron Dicker</td>\n      <td>ENTERTAINMENT</td>\n      <td>2018-05-26</td>\n      <td>Hugh Grant Marries For The First Time At Age 57</td>\n      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ron Dicker</td>\n      <td>ENTERTAINMENT</td>\n      <td>2018-05-26</td>\n      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n      <td>The actor gives Dems an ass-kicking for not fi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ron Dicker</td>\n      <td>ENTERTAINMENT</td>\n      <td>2018-05-26</td>\n      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n      <td>The \"Dietland\" actress said using the bags is ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d390c727f3c433b75245716c4820c1e22f76f54f"
      },
      "cell_type": "code",
      "source": "cates = df.groupby('category')\nprint(\"total categories:\", cates.ngroups)\nprint(cates.size())",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "total categories: 31\ncategory\nARTS               1509\nARTS & CULTURE     1339\nBLACK VOICES       3858\nBUSINESS           4254\nCOLLEGE            1144\nCOMEDY             3971\nCRIME              2893\nEDUCATION          1004\nENTERTAINMENT     14257\nFIFTY              1401\nGOOD NEWS          1398\nGREEN              2622\nHEALTHY LIVING     6694\nIMPACT             2602\nLATINO VOICES      1129\nMEDIA              2815\nPARENTS            3955\nPOLITICS          32739\nQUEER VOICES       4995\nRELIGION           2556\nSCIENCE            1381\nSPORTS             4167\nSTYLE              2254\nTASTE              2096\nTECH               1231\nTHE WORLDPOST      3664\nTRAVEL             2145\nWEIRD NEWS         2670\nWOMEN              3490\nWORLD NEWS         2177\nWORLDPOST          2579\ndtype: int64\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62b312455273454444afcf4a9f80f2c6e6fb3b70"
      },
      "cell_type": "code",
      "source": "df['text'] = df.headline + \" \" + df.short_description",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71f865a412909c13dc87087dc07f04a7ec6478cb"
      },
      "cell_type": "code",
      "source": "tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.text)\nX = tokenizer.texts_to_sequences(df.text)\ndf['words'] = X",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5f142592ed1b1c29f7e4facc44f9c012e1a3485"
      },
      "cell_type": "code",
      "source": "maxlen = max([len(t) for t in df['words']]);maxlen",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "248"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "346ed6b0ea2da9db671369daf169707d8e363302"
      },
      "cell_type": "code",
      "source": "df['word_length'] = df.words.apply(lambda i: len(i))",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e0396bc679252c4a31723489b600b6511fa43b9c"
      },
      "cell_type": "code",
      "source": "df.word_length.describe()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "count    124989.000000\nmean         25.959844\nstd          14.446685\nmin           0.000000\n25%          17.000000\n50%          24.000000\n75%          32.000000\nmax         248.000000\nName: word_length, dtype: float64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "675605a4473b89580101989404d2a4de04c1d832"
      },
      "cell_type": "code",
      "source": "X = list(sequence.pad_sequences(df.words, maxlen=maxlen))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7babce5169d6b91c1706d56c51f98775b8476766"
      },
      "cell_type": "code",
      "source": "categories = df.groupby('category').size().index.tolist()",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfe7270d0d2dfcab813cf64220b9aaa906529037"
      },
      "cell_type": "code",
      "source": "category_int = {}\nint_category = {}\nfor i, k in enumerate(categories):\n    category_int.update({k:i})\n    int_category.update({i:k})\n\ndf['c2id'] = df['category'].apply(lambda x: category_int[x])",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f1f3b556b586d3c0cd4ad602effb0bc379a2c18"
      },
      "cell_type": "code",
      "source": "X = np.array(X)\nY = np_utils.to_categorical(list(df.c2id))",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf5cd3c5c99c3cda0a3394b1167e4cc759e62608"
      },
      "cell_type": "code",
      "source": "seed = 29\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=seed)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b01c0861cd1fc5982c7b7cd45103fbde0768f25b"
      },
      "cell_type": "code",
      "source": "class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    ",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf9ecea90f4454486e996e8efc3dc7ed7912bdbd"
      },
      "cell_type": "code",
      "source": "word_index = tokenizer.word_index\n\nEMBEDDING_DIM = 100\n\nembeddings_index = {}\nf = open('../input/glove6b/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s unique tokens.' % len(word_index))\nprint('Total %s word vectors.' % len(embeddings_index))\n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembedding_layer = Embedding(len(word_index)+1,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=maxlen,\n                            trainable=False)",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found 86627 unique tokens.\nTotal 400000 word vectors.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e51533bbef50f7157e9005964753a947610abd0a"
      },
      "cell_type": "code",
      "source": "inp = Input(shape=(maxlen,), dtype='int32')\nembedding= embedding_layer(inp)\nbilstm_layer = Bidirectional(LSTM(150, dropout=0.25, recurrent_dropout=0.25, return_sequences=True))\nx = bilstm_layer(embedding)\nx = Dropout(0.25)(x)\nmerged = Attention(maxlen)(x)\nmerged = Dropout(0.25)(merged)\nmerged = BatchNormalization()(merged)\noutp = Dense(len(int_category), activation='softmax')(merged)\n\nAttentionLSTM = Model(inputs=inp, outputs=outp)\nAttentionLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n\nAttentionLSTM.summary()",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 248)               0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 248, 100)          8662800   \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 248, 300)          301200    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 248, 300)          0         \n_________________________________________________________________\nattention_1 (Attention)      (None, 300)               548       \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 300)               0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 300)               1200      \n_________________________________________________________________\ndense_1 (Dense)              (None, 31)                9331      \n=================================================================\nTotal params: 8,975,079\nTrainable params: 311,679\nNon-trainable params: 8,663,400\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f57525313e9135108ad808efc58127fe043c30fd"
      },
      "cell_type": "code",
      "source": "attlstm_history = AttentionLSTM.fit(x_train, \n                                    y_train, \n                                    batch_size=128, \n                                    epochs=20, \n                                    validation_data=(x_val, y_val))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ada01e1d51bfeb01eb5a976151af3ed228e2137e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}